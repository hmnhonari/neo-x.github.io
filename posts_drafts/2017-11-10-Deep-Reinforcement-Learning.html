<p>Reinforcemen learning is large and accelerating area of research. The recent advances in combining RL method with Deep learning have given way to solutions to challenging problems <a href="https://www.nature.com/articles/nature14236">Like playing Atari</a> and <a href="https://www.bloomberg.com/features/2015-preschool-for-robots/">Robotic Manipulation</a>. These advances have been wonderful but as many practitioners might have relized getting these methods to work together is less than trivial. There is a <a href="http://incompleteideas.net/sutton/book/the-book.html">great book</a> on RL by Richard Sutton that covers the general method in RL. There is also another book on <a href="http://www.deeplearningbook.org/">Deep learning</a> but in my opinon there my never be enoug recources on how to combine these two methods. In these articles I hope to put together a colection of material that will guide the reader along a path understanding and implimenting a method that could <a href="https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html">solve Go</a> or <a href="http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/index.html">learn to walk</a>.</p>

<p>A large goal of these articles is to try and explain the concepts in relatively simple terms. When broken down into small parts RL can be very understandable. However, There can be much confusion in the area for a number of reasons, many definitions have multiple names that are all the same thing, the discourse assumes some Machine Learning (ML) prior knowledge, the explination is too breif, etc… I learned these concepts coming from a computer animation background, they were all very new to me. I however like to visualize and animation things to help me understand what is happening. I hope these additional aids assist in the learning process.</p>

<p>Try and explain things with integrals and finite sums…</p>

<h2 id="list-of-topics">List of topics</h2>

<ol>
  <li>Intro to RL (first few chapters of Suttons book</li>
  <li>Markov Decision Process</li>
  <li>Future Discounted Reward (and trajectories)</li>
  <li>Policies and Value functions</li>
  <li>Optimal Policies and Value functions</li>
  <li>Policy Improvement (policy iteration and value iteration)</li>
  <li></li>
</ol>

