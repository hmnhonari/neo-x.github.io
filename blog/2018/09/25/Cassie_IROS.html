<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> | Glen Berseth</title>
    <link rel="canonical" href="/blog/2018/09/25/Cassie_IROS.html">
    

    <meta property="og:title" content="">
    <meta property="og:url" content="/blog/2018/09/25/Cassie_IROS.html">
    <meta property="og:site_name" content="Glen Berseth">
    <meta name="twitter:card" content="summary">
    <meta property="twitter:title" content="">

	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
    <script type="application/ld+json">
    {"@type":"WebSite","headline":"Glen Berseth","url":"/","name":"Glen Berseth","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- <link rel="stylesheet" href="/assets/main.css"> --></head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Glen Berseth</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path d="M1.5 1.5 h15 M1.5 7.5 h15 M1.5 13.5 h15" stroke="black" stroke-width="3" stroke-linecap="round" />
                      </svg>
                </span>
            </label>

            <div class="trigger">
                    <a class="page-link" href="/">Home</a>
                
                    <a class="page-link" href="/people/">People</a>
                
                    <a class="page-link" href="/publications/">Publications</a>
                
                    <a class="page-link" href="/join/">Join Us</a>
                
                    <a class="page-link" href="/posts/">Posts</a>
                
                    <a class="page-link" href="/teaching/">Teaching</a>
                
                <a class="page-link" href="/fr/blog/2018/09/25/Cassie_IROS.html">Fr</a>
                
            </div>
        </nav></div>
    
</header>
<main class="page-content" aria-label="Content">
        <div class="wrapper">
            <article class="post">

    <header class="post-header">
        <h1 class="post-title"></h1>
    </header>
    <div class="post-content">
        <div align="center">
	<p>	
            University of British Columbia
    </p>
</div>

<div align="center">
			<span class="STYLE17"> <img width="200" src="/assets/projects/Cassie_IROS/teaser2.png" /> </span>
			<span class="STYLE17"> <img width="200" src="/assets/projects/Cassie_IROS/teaser.png" /> </span>
			<span class="STYLE17"> <img width="350" src="/assets/projects/Cassie_IROS/teaser3.png" /> </span>
			 &nbsp; &nbsp; &nbsp;

</div>

<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018)</p>

<p>Zhaoming Xie (1), Glen Berseth (1), Patrick Clary (2), Jonathan Hurst (2), Michiel van de Panne (1)</p>

<p>(1) University of British Columbia
(2) Oregon State University</p>

<h1 id="abstract">Abstract</h1>

<p>Bipedal locomotion skills are challenging to develop. Control strategies often use local linearization of the dynamics in conjunction with reduced-order abstractions to yield tractable solutions. In these model-based control strategies, the controller is often not fully aware of many details, including torque limits, joint limits, and other non-linearities that are necessarily excluded from the control computations for simplicity. Deep reinforcement learning (DRL) offers a promising model-free approach for controlling bipedal locomotion which can more fully exploit the dynamics. However, current results in the machine learning literature are often based on ad-hoc simulation models that are not based on corresponding hardware. Thus it remains unclear how well DRL will succeed on realizable bipedal robots. In this paper, we demonstrate the effectiveness of DRL using a realistic model of Cassie, a bipedal robot. By formulating a feedback control problem as finding the optimal policy for a Markov Decision Process, we are able to learn robust walking controllers that imitate a reference motion with DRL. Controllers for different walking speeds are learned by imitating simple time- scaled versions of the original reference motion. Controller robustness is demonstrated through several challenging tests, including sensory delay, walking blindly on irregular terrain and unexpected pushes at the pelvis. We also show we can interpolate between individual policies and that robustness can be improved with an interpolated policy.</p>

<h2 id="files">Files</h2>

<p><a href="/assets/projects/Cassie_IROS/2018-IROS-cassie.pdf">Paper</a></p>

<h2 id="videos">Videos!</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/z3DMKQwt68Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<p>On real robot</p>

<iframe width="560" height="315" src="cassie.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<h3 id="bibtex">Bibtex</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{2018-IROS-cassie,
  title={Feedback Control for Cassie with Deep Reinforcement Learning},
  author={Zhaoming Xie and Glen Berseth and Patrick Clary and Jonathan Hurst and Michiel van de Panne},
  booktitle = {Proc. IEEE/RSJ Intl Conf on Intelligent Robots and Systems (IROS 2018)},
  year={2018}
}
</code></pre></div></div>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We thank the anonymous reviewers for their helpful feedback. This research was funded in part by an NSERC Discovery Grant (RGPIN-2015-04843).</p>


    </div>

</article>
        </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">
        

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li class="p-name">Glen Berseth</li></ul>
            </div>

            <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/neo-x"><svg
                class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#github"></use>
            </svg> <span class="username">neo-x</span></a></li><li><a
            href="https://www.linkedin.com/in/glen-berseth-0523278b"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
            </svg> <span class="username">glen-berseth-0523278b</span></a></li><li><a
            href="https://www.twitter.com/GlenBerseth"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
            </svg> <span class="username">GlenBerseth</span></a></li><li><a rel="me"
            href="https://www.youtube.com/channel/UCOouaBg4gHIlNvPkJn_8ooA"
            title="Real Lab"><svg class="svg-icon grey">
                <use xlink:href="/assets/minima-social-icons.svg#youtube"></use>
            </svg> <span class="username">Real Lab</span></a></li></ul></div>

            <div class="footer-col footer-col-3">
                <p>I (he/him/il) am an assistant professor at the University de Montreal and Mila. My research explores how to use deep learning and reinforcement learning to develop generalist robots.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>