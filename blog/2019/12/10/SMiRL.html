<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> | Glen Berseth</title>
    <link rel="canonical" href="/blog/2019/12/10/SMiRL.html">
    

    <meta property="og:title" content="">
    <meta property="og:url" content="/blog/2019/12/10/SMiRL.html">
    <meta property="og:site_name" content="Glen Berseth">
    <meta name="twitter:card" content="summary">
    <meta property="twitter:title" content="">

	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
    <script type="application/ld+json">
    {"@type":"WebSite","headline":"Glen Berseth","url":"/","name":"Glen Berseth","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- <link rel="stylesheet" href="/assets/main.css"> --></head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Glen Berseth</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path d="M1.5 1.5 h15 M1.5 7.5 h15 M1.5 13.5 h15" stroke="black" stroke-width="3" stroke-linecap="round" />
                      </svg>
                </span>
            </label>

            <div class="trigger">
                    <a class="page-link" href="/">Home</a>
                
                    <a class="page-link" href="/people/">People</a>
                
                    <a class="page-link" href="/publications/">Publications</a>
                
                    <a class="page-link" href="/join/">Join Us</a>
                
                    <a class="page-link" href="/posts/">Posts</a>
                
                    <a class="page-link" href="/teaching/">Teaching</a>
                
                <a class="page-link" href="/fr/blog/2019/12/10/SMiRL.html">Fr</a>
                
            </div>
        </nav></div>
    
</header>
<main class="page-content" aria-label="Content">
        <div class="wrapper">
            <article class="post">

    <header class="post-header">
        <h1 class="post-title"></h1>
    </header>
    <div class="post-content">
        <div align="center">
	<p>
				Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dinesh Jayaraman, Sergey Levine
	</p>
	<p>	
            UC Berkeley, Stanford, Facebook
    </p>
</div>

<h2 id="emergent-behavior-by-minimizing-chaos">Emergent Behavior by Minimizing Chaos</h2>

<p>All living organisms carve out environmental niches within which they can maintain relative predictability amidst the ever-increasing entropy around them [<a href="http://www.ler.esalq.usp.br/aulas/lce1302/life_as_a_manifestation.pdf">1</a>, <a href="https://www.fil.ion.ucl.ac.uk/~karl/The free-energy principle - a rough guide to the brain.pdf">2</a>]. Humans, for example, go to great lengths to shield themselves from surprise — we band together in millions to build cities with homes, supplying water, food, gas, and electricity to control the deterioration of our bodies and living spaces amidst heat and cold, wind and storm. The need to discover and maintain such surprise-free equilibria has driven great resourcefulness and skill in organisms across very diverse natural habitats. Motivated by this, we ask: could the motive of preserving order amidst chaos guide the automatic acquisition of useful behaviors in artificial agents?</p>

<p>How might an agent in an environment acquire complex behaviors and skills with no external supervision? This central problem in artificial intelligence has evoked several candidate solutions, largely focusing on novelty-seeking behaviors [<a href="http://people.idsia.ch/~juergen/curioussingapore/curioussingapore.html">3</a>, <a href="https://arxiv.org/abs/1606.01868">4</a>, <a href="https://pathak22.github.io/noreward-rl/">5</a>]. In simulated worlds, such as video games, novelty-seeking intrinsic motivation can lead to interesting and meaningful behavior. However, these environments may be fundamentally lacking compared to the real world. In the real world, natural forces and other agents offer bountiful novelty. Instead, the challenge in natural environments is allostasis: discovering behaviors that enable agents to maintain an equilibrium (homeostasis), for example to preserve their bodies, their homes, and avoid predators and hunger. In the example below we shown an example where an agent is experiencing random events due to the changing weather. If the agent learns to build a shelter, in this case a house, the agent will reduce the observed effects from weather.</p>

<div align="center">
            <img width="600" src="/assets/projects/SMiRL/robotsurprise_stacked.png" />
</div>

<p>We formalize homeostasis as an objective for reinforcement learning based on surprise minimization (SMiRL). In entropic and dynamic environments with undesirable forms of novelty, minimizing surprise (i.e., minimizing novelty) causes agents to naturally seek an equilibrium that can be stably maintained.</p>

<div align="center">
     <img width="600" src="/assets/projects/SMiRL/SMiRL_Outline.png" />
</div>

<table>
  <tbody>
    <tr>
      <td>Here we show an illustration of the agent interaction loop using SMiRL. When the agent observes a state $\textbf{s}$, it computes the probability of this new state given the belief the agent has $r_{t} \leftarrow p_{\theta_{t-1}}(\textbf{s})$. This belief models the states the agent is most familiar with – i.e., the distribution of states it has seen in the past. Experiencing states that are more familiar will result in higher reward. After the agent experience a new state it updates its belief $p_{\theta_{t-1}}(\textbf{s})$ over states to include the most recent experience. Then, the goal of the action policy $\pi(a</td>
      <td>\textbf{s}, \theta_{t})$ is to choose actions that will result in the agent consistently experiencing familiar states. Crucially, the agent understands that its beliefs will change in the future. This means that it has two mechanisms by which to maximize this reward: taking actions to visit familiar states, and taking actions to visit states that will <em>change its beliefs</em> such that future states are more familiar. It is this latter mechanism that results in complex emergent behavior. Below, we visualize a policy trained to play the game of Tetris. On the left the blocks the agent chooses are shown and on the right is a visualization of $p_{\theta_{t}}(\textbf{s})$. We can see how as the episode progresses the belief over possible locations to place blocks tends to favor only the bottom row. This encourages the agent to eliminate blocks to prevent board from filling up.</td>
    </tr>
  </tbody>
</table>

<div class="t">
    <table align="center">
        <tr>
    <td align="center">
        <img width="200" src="/assets/projects/SMiRL/tetris/tetris_ps.gif" />
        </td>
    <td>
    <img width="320" src="/assets/projects/SMiRL/miniGrid/minigrid-maze-random-count.gif" />
           </td>
	</tr>
        <tr align="center">
        <td>
            Tetris
            </td>
        <td>
            HauntedHouse
            </td>
        </tr>
</table>
</div>

<h4 id="emergent-behavior">Emergent behavior</h4>

<p>The SMiRL agent demonstrates meaningful emergent behaviors in a number of different environments. In the Tetris environment, the agent is able to learn proactive behaviors to eliminate rows and properly play the game. The agent also learns emergent game playing behavior in the VizDoom environment, acquiring an effective policy for dodging the fireballs thrown by the enemies. In both of these environments, stochastic and chaotic events force the SMiRL agent to take a coordinated course of action to avoid unusual states, such as full Tetris boards or fireball explorations.</p>

<div class="t">
    <table align="center">
        <tr>
    <td>
        <img width="100%" src="/assets/projects/SMiRL/vizdoom/Doom_trained_enough_result.gif" />
        </td>
    <td>
    <img width="100%" src="/assets/projects/SMiRL/vizdoom/vizdoom_dtl.gif" />
           </td>
	</tr>
        <tr align="center">
        <td>
            Doom Hold The Line
            </td>
        <td>
            Doom Defend The Line
            </td>
        </tr>
</table>
</div>

<h5 id="biped">Biped</h5>

<p>In the Cliff environment, the agent learns a policy that greatly reduces the probability of falling off of the cliff by bracing against the ground and stabilize itself at the edge, as shown in the figure below. In the <em>Treadmill</em> environment, SMiRL learns a more complex locomotion behavior, jumping forward to increase the time it stays on the treadmill, as shown in figure below.</p>

<div class="t">
    <table align="center">
        <tr>
    <td>
        <video width="320" height="240" autoplay="">   <source src="/assets/projects/SMiRL/biped/cliff_surpise_VAE_6_v3_rewardViz.mp4" type="video/mp4" />   <source src="movie.ogg" type="video/ogg" /> Your browser does not support the video tag. </video>
        </td>
    <td>
    <video width="320" height="240" autoplay="">   <source src="/assets/projects/SMiRL/biped/treadmill_surpise_VAE_6_v3_rewardViz.mp4" type="video/mp4" />   <source src="movie.ogg" type="video/ogg" /> Your browser does not support the video tag. </video>
           </td>
	</tr>
        <tr align="center">
        <td>
            Cliff
            </td>
        <td>
            Treadmill
            </td>
        </tr>
</table>
</div>

<p>​</p>

<h4 id="comparison-to-intrinsic-motivation">Comparison to Intrinsic motivation:</h4>

<p>Intrinsic motivation is the idea that behavior is driven by internal reward signals that are task independent. Below, we show plots of the environment-specific rewards over time on Tetris, VizDoomTakeCover, and the humanoid domains. In order to compare SMiRL to more standard intrinsic motivation methods, which seek out states that maximize surprise or novelty, we also evaluated ICM [<a href="https://pathak22.github.io/noreward-rl/">5</a>] and RND [<a href="https://arxiv.org/abs/1810.12894">6</a>]. We include an oracle agent that directly optimizes the task reward. On Tetris, after training for $2000$ epochs, SMiRL achieves near perfect play, on par with the oracle reward optimizing agent, with no deaths. ICM seeks novelty by creating more and more distinct patterns of blocks rather than clearing them, leading to deteriorating game scores over time. On VizDoomTakeCover, SmiRL effectively learns to dodge fireballs thrown by the adversaries.</p>

<div align="center">
     <img width="90%" src="/assets/projects/SMiRL/video_game_comparisons_2.png" />
</div>

<p>The baseline comparisons for the Cliff and Treadmill environments have a similar outcome. The novelty seeking behavior of ICM causes it to learn a type of irregular behavior that causes the agent to jump off the Cliff and roll around on the Treadmill, maximizing the variety (and quantity) of falls.</p>

<h4 id="smirl--curiosity">SMiRL + Curiosity:</h4>

<div align="center">
     <img width="90%" src="/assets/projects/SMiRL/Capture_biped_results.png" />
</div>

<p>While on the surface, SMiRL minimizes surprise and curiosity approaches like ICM maximize novelty, they are in fact not mutually incompatible. In particular, while ICM maximizes novelty with respect to a learned transition model, SMiRL minimizes surprise with respect to a learned state distribution. We can combine ICM and SMiRL to achieve even better results on the Treadmill environment.</p>

<div class="t">
    <table align="center">
        <tr>
    <td>
        <video width="320" height="240" autoplay="">   <source src="/assets/projects/SMiRL/biped/treadmill_surpise_ICM_v3_rewardViz.mp4" type="video/mp4" />   <source src="movie.ogg" type="video/ogg" /> Your browser does not support the video tag. </video>
        </td>
    <td>
    <video width="320" height="240" autoplay="">   <source src="/assets/projects/SMiRL/biped/pedistal_surpise_v3_rewardViz.mp4" type="video/mp4" />   <source src="movie.ogg" type="video/ogg" /> Your browser does not support the video tag. </video>
           </td>
	</tr>
        <tr align="center">
        <td>
            Treadmill + ICM
            </td>
        <td>
            Pedestal
            </td>
        </tr>
</table>
</div>

<h4 id="insights">Insights:</h4>

<p>The key insight utilized by our method is that, in contrast to simple simulated domains, realistic environments exhibit dynamic phenomena that gradually increase entropy over time. An agent that resists this growth in entropy must take active and coordinated actions, thus learning increasingly complex behaviors. This is different from commonly proposed intrinsic exploration methods based on novelty, which instead seek to visit novel states and increase entropy. SMiRL holds promise for a new kind of unsupervised RL method that produces behaviors that are closely tied to the prevailing disruptive forces, adversaries, and other sources of entropy in the environment.</p>

<p>This post is based on the following paper:</p>

<ul>
  <li>Glen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman, Sergey Levine. <br />
<a href="https://arxiv.org/abs/1912.05510">SMiRL: Surprise Minimizing RL in Dynamic Environments</a> <br />
<a href="https://sites.google.com/view/surpriseminimization">Project Website</a></li>
</ul>

    </div>

</article>
        </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">
        

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li class="p-name">Glen Berseth</li></ul>
            </div>

            <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/neo-x"><svg
                class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#github"></use>
            </svg> <span class="username">neo-x</span></a></li><li><a
            href="https://www.linkedin.com/in/glen-berseth-0523278b"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
            </svg> <span class="username">glen-berseth-0523278b</span></a></li><li><a
            href="https://www.twitter.com/GlenBerseth"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
            </svg> <span class="username">GlenBerseth</span></a></li><li><a rel="me"
            href="https://www.youtube.com/channel/UCOouaBg4gHIlNvPkJn_8ooA"
            title="Real Lab"><svg class="svg-icon grey">
                <use xlink:href="/assets/minima-social-icons.svg#youtube"></use>
            </svg> <span class="username">Real Lab</span></a></li></ul></div>

            <div class="footer-col footer-col-3">
                <p>I (he/him/il) am an assistant professor at the University de Montreal and Mila. My research explores how to use deep learning and reinforcement learning to develop generalist robots.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>