<!DOCTYPE html>
<html lang="fr"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Accueil | Glen Berseth</title>
    <link rel="canonical" href="/">
    

    <meta property="og:title" content="Accueil">
    <meta property="og:url" content="/">
    <meta property="og:site_name" content="Glen Berseth">
    <meta name="twitter:card" content="summary">
    <meta property="twitter:title" content="Accueil">

	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
    <script type="application/ld+json">
    {"@type":"WebSite","headline":"Glen Berseth","url":"/","name":"Glen Berseth","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- <link rel="stylesheet" href="/assets/main.css"> --></head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Glen Berseth</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path d="M1.5 1.5 h15 M1.5 7.5 h15 M1.5 13.5 h15" stroke="black" stroke-width="3" stroke-linecap="round" />
                      </svg>
                </span>
            </label>

            <div class="trigger">
                    <a class="page-link" href="/fr/">Accueil</a>
                
                    <a class="page-link" href="/fr/people/">Membres</a>
                
                    <a class="page-link" href="/fr/publications/">Publications</a>
                
                    <a class="page-link" href="/fr/join/">Joignez-nous</a>
                
                    <a class="page-link" href="/fr/posts/">Postes</a>
                
                    <a class="page-link" href="/fr/teaching/">L'enseignement</a>
                
                <a class="page-link" href="/">En</a>
                
            </div>
        </nav></div>
    
</header>
<main class="page-content" aria-label="Content">
        <div class="wrapper">
            <article class="post">

    <header class="post-header">
        <h1 class="post-title">Accueil</h1>
    </header>
    <div class="post-content">
        <div align="center">
    <table align="center">
        <tr>
            <td width="25%">
                <img width="100%" src="/assets/images/bios/glen-berseth.png" />
            </td>
            <td width="75%">
                Je suis professeur adjoint à l'<a href="https://diro.umontreal.ca/accueil/">Université de Montréal</a>, membre académique principal du <a href="https://mila.quebec/en/">Mila - Institut québécois de l'intelligence artificielle</a>, titulaire de la <a href="https://cifar.ca/ai/canada-cifar-ai-chairs/">Chaire d'IA Canada CIFAR</a>, et co-directeur du <a href="https://montrealrobotics.ca/">Laboratoire de Robotique et d'IA Incarnée (REAL)</a>.
                J'ai été chercheur post-doctoral au sein de <a href="https://bair.berkeley.edu/">la Recherche en Intelligence Artificielle de Berkeley (BAIR)</a>, travaillant avec <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>.
                Mes recherches passées et actuelles sont axées sur la résolution de problèmes de prise de décision séquentielle pour les systèmes d'apprentissage autonome du monde réel (robots).
                Plus précisément, mes recherches ont couvert les domaines de l'apprentissage par renforcement, continu, méta, hiérarchique et la collaboration homme-robot.
                Dans le cadre de mon travail, j'ai publié dans les principales disciplines de la robotique, de l'apprentissage automatique et de l'animation informatique.
                Actuellement, j'enseigne un cours sur l'apprentissage des robots à l'Université de Montréal et au Mila, qui couvre les recherches les plus récentes sur les techniques d'apprentissage automatique pour la création de robots généralistes.

                <p>Pour une biographie plus formelle, cliquez <a href="biography/bio.html">ici</a>.</p>
            </td>
        </tr>
    </table>
</div>

<p>Je soutiens <a href="http://slow-science.org/">Slow Science</a>.</p>



<h2>Nouvelles</h2>






<ul>
  <li>
    <p>(janvier 2025) Trois nouveaux articles acceptés à NeurIPS 2024 sur l’inverse safe RL, la conversion et la mise à l’échelle des algorithmes deepRL, et l’amortissement de l’inférence intraitable !</p>
  </li>
  <li>
    <p>(novembre 2024) Trois nouveaux articles acceptés à NeurIPS 2024 sur l’inverse safe RL, la conversion et la mise à l’échelle des algorithmes deepRL, et l’amortissement de l’inférence intraitable !</p>
  </li>
  <li>
    <p>(août 2024) La première &lt;a href=« https://rl-conference.cc/ »&gt;Conférence sur l’apprentissage par renforcement&lt;/a&gt; a été un véritable succès!</p>
  </li>
  <li>
    <p>(juin 2024) New paper accepted to RSS 2024 on <a href="https://droid-dataset.github.io/">Making more diverse data sets for improving generalist robot policies</a>!</p>
  </li>
  <li>
    <p>(mai 2024) Best paper award at ICLR to ICLR 2024 on <a href="https://robotics-transformer-x.github.io/">VLMs for robotics</a>!</p>
  </li>
  <li>
    <p>(janvier 2024) Five new papers accepted to ICLR 2024 on <a href="https://arxiv.org/abs/2310.18144">efficient exploration</a>, <a href="https://arxiv.org/abs/2401.11237">generalization in sequence planning for supervised learning</a>, <a href="https://openreview.net/forum?id=Nq45xeghcL">Intelligent Switching for Reset-Free RL</a>, <a href="https://arxiv.org/abs/2310.02902">Using LLMs and RL for drug discovery</a>, and <a href="https://arxiv.org/abs/2309.06599">Latent diffusion for OfflineRL</a>!</p>
  </li>
  <li>
    <p>(septembre 2023) New paper accepted to NeurIPS 2023 on <a href="https://arxiv.org/abs/2306.14808">efficient exploration in finite horizons</a>!</p>
  </li>
  <li>
    <p>(septembre 2023) New paper accepted to <a href="https://arxiv.org/abs/2309.03839">iROS</a> on using OfflineRL to bootstrap human robot interfaces!</p>
  </li>
  <li>
    <p>(août 2023) New paper accepted to <a href="https://ieeexplore.ieee.org/document/10214627">RA-L</a> on the benefits of torque control over position control for humanoid robots and Sim2Real training!</p>
  </li>
  <li>
    <p>(juin 2022) q</p>
  </li>
  <li>
    <p>(septembre 2021) q</p>
  </li>
  <li>
    <p>(avril 2021) q</p>
  </li>
</ul>




<h2>Research Talk (2023)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y20ICd1voEM?si=eCA1vVwSlgEHS9Qt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2>Recent Research Talk (2023)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/-7sctG9f1As?si=h9WnQrip-8_vc0Tk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2 id="travaux-récents">Travaux Récents</h2>

<div align="center">
    <table align="center">
        <tr>
            <td width="33%">
                <a href="https://sites.google.com/view/relmm">
                    <h4>Apprentissage par renforcement autonome en conditions réelles</h4>
                    <img width="100%" src="/assets/projects/ReLMM/complex_room_short.gif" />
                </a>
            </td>
            <td width="66%">
                Nous étudions comment les robots peuvent apprendre de manière autonome des compétences nécessitant à la fois la navigation et la préhension. Bien que l'apprentissage par renforcement permette en principe l'apprentissage automatisé des compétences robotiques, dans la pratique, l'apprentissage par renforcement dans le monde réel est complexe et nécessite souvent une instrumentation et une supervision importantes. Notre objectif est de concevoir un système d'apprentissage par renforcement robotique pour apprendre la navigation et la manipulation ensemble, de manière autonome et sans intervention humaine, permettant un apprentissage continu dans des conditions réalistes. Notre système proposé, ReLMM, peut apprendre en continu sur une plateforme du monde réel sans instrumentation environnementale, sans intervention humaine et sans accès à des informations privilégiées, telles que des cartes, des positions d'objets ou une vue globale de l'environnement. Notre méthode utilise une politique modularisée avec des composants pour la manipulation et la navigation, où l'incertitude de la politique de manipulation guide l'exploration pour le contrôleur de navigation, et le module de manipulation fournit des récompenses pour la navigation. Nous évaluons notre méthode sur une tâche de nettoyage de pièce, où le robot doit se déplacer et ramasser des objets dispersés sur le sol. Après une phase d'entraînement de curriculum de préhension, ReLMM peut apprendre la navigation et la préhension ensemble de manière entièrement automatique, en environ 40 heures d'entraînement autonome en conditions réelles.

            </td>
        </tr>
    </table>
</div>

<div align="center">
    <table align="center">
        <tr>
            <td width="33%">
                <a href="https://arxiv.org/abs/2104.11707">
                    <h4>Généralisation entre robots par inférence de la morphologie</h4>
                    <img width="100%" src="/assets/projects/anymorph/anymorph_prompt.gif" />
                </a>
            </td>
            <td width="66%">
                L'approche prototypique de l'apprentissage par renforcement consiste à former des politiques adaptées à un agent particulier à partir de zéro pour chaque nouvelle morphologie. Les travaux récents visent à éliminer la rétroaction des politiques en examinant si une politique agnostique à la morphologie, formée sur un ensemble diversifié d'agents ayant des objectifs de tâche similaires, peut être transférée à de nouveaux agents ayant des morphologies inconnues sans rétroformation. Il s'agit d'un problème complexe qui a nécessité que les approches précédentes utilisent des descriptions de la morphologie du nouvel agent préalablement conçues. Au lieu de concevoir manuellement cette description, nous proposons une méthode basée sur les données qui apprend une représentation de la morphologie directement à partir de l'objectif de l'apprentissage par renforcement. Notre approche est le premier algorithme d'apprentissage par renforcement capable de former une politique capable de se généraliser à de nouvelles morphologies d'agent sans nécessiter une description préalable de la morphologie de l'agent. Nous évaluons notre approche sur la référence standard pour le contrôle agnostique de l'agent et améliorons l'état actuel de l'art en généralisation sans formation préalable à de nouveaux agents. Importamment, notre méthode atteint de bonnes performances sans description explicite de la morphologie.

            </td>
        </tr>
    </table>
</div>

<div align="center">
    <table align="center">
        <tr>
            <td width="33%">
                <a href="https://sites.google.com/view/surpriseminimization">
                    <h4>Minimisation de l'entropie pour des comportements émergents</h4>
                    <img width="100%" src="/assets/projects/SMiRL/robotsurprise_stacked.png" />
                    <img width="100%" src="/assets/projects/SMiRL/miniGrid/minigrid-maze-random-count.gif" />
                </a>
            </td>
            <td width="66%">
                Tous les organismes vivants définissent des niches environnementales dans lesquelles ils peuvent maintenir une prédictibilité relative au milieu de l'entropie croissante qui les entoure [schneider1994, friston2009]. Les êtres humains, par exemple, font de grands efforts pour se protéger de la surprise : nous nous regroupons par millions pour construire des villes avec des maisons, fournissant de l'eau, de la nourriture, du gaz et de l'électricité pour contrôler la détérioration de nos corps et de nos espaces de vie face à la chaleur et au froid, au vent et à la tempête. Le besoin de découvrir et de maintenir de tels équilibres sans surprise a suscité beaucoup d'ingéniosité et d'habileté chez les organismes dans des habitats naturels très divers. Motivés par cela, nous nous demandons si le désir de préserver l'ordre au milieu du chaos pourrait guider l'acquisition automatique de comportements utiles chez les agents artificiels.

            </td>
        </tr>
    </table>
</div>


    </div>

</article>
        </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">
        

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li class="p-name">Glen Berseth</li></ul>
            </div>

            <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/neo-x"><svg
                class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#github"></use>
            </svg> <span class="username">neo-x</span></a></li><li><a
            href="https://www.linkedin.com/in/glen-berseth-0523278b"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
            </svg> <span class="username">glen-berseth-0523278b</span></a></li><li><a
            href="https://www.twitter.com/GlenBerseth"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
            </svg> <span class="username">GlenBerseth</span></a></li><li><a rel="me"
            href="https://www.youtube.com/channel/UCOouaBg4gHIlNvPkJn_8ooA"
            title="Real Lab"><svg class="svg-icon grey">
                <use xlink:href="/assets/minima-social-icons.svg#youtube"></use>
            </svg> <span class="username">Real Lab</span></a></li></ul></div>

            <div class="footer-col footer-col-3">
                <p>I (he/him/il) am an assistant professor at the University de Montreal and Mila. My research explores how to use deep learning and reinforcement learning to develop generalist robots.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>