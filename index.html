<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Home | Glen Berseth</title>
    <link rel="canonical" href="/">
    

    <meta property="og:title" content="Home">
    <meta property="og:url" content="/">
    <meta property="og:site_name" content="Glen Berseth">
    <meta name="twitter:card" content="summary">
    <meta property="twitter:title" content="Home">

	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
    <script type="application/ld+json">
    {"@type":"WebSite","headline":"Glen Berseth","url":"/","name":"Glen Berseth","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- <link rel="stylesheet" href="/assets/main.css"> --></head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Glen Berseth</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path d="M1.5 1.5 h15 M1.5 7.5 h15 M1.5 13.5 h15" stroke="black" stroke-width="3" stroke-linecap="round" />
                      </svg>
                </span>
            </label>

            <div class="trigger">
                    <a class="page-link" href="/">Home</a>
                
                    <a class="page-link" href="/people/">People</a>
                
                    <a class="page-link" href="/publications/">Publications</a>
                
                    <a class="page-link" href="/join/">Join Us</a>
                
                    <a class="page-link" href="/posts/">Posts</a>
                
                    <a class="page-link" href="/teaching/">Teaching</a>
                
                <a class="page-link" href="/fr/">Fr</a>
                
            </div>
        </nav></div>
    
</header>
<main class="page-content" aria-label="Content">
        <div class="wrapper">
            <article class="post">

    <header class="post-header">
        <h1 class="post-title">Home</h1>
    </header>
    <div class="post-content">
        <div align="center">     <table align="center">        <tr>    <td width="25%">   <img width="100%" src="/assets/images/bios/glen-berseth.png" /> </td> <td width="75%">  
I am an assistant professor at the <a href="https://diro.umontreal.ca/accueil/">Université de Montréal</a>, a core academic member of the <a href="https://mila.quebec/en/">Mila - Quebec AI Institute</a>, <a href="https://cifar.ca/ai/canada-cifar-ai-chairs/">Canada CIFAR AI chair</a>, member <a href="https://institut-courtois.umontreal.ca/">L'Institut Courtois</a>, and co-director of the <a href="https://montrealrobotics.ca/">Robotics and Embodied AI Lab (REAL)</a>. 
    I was a postdoctoral researcher with <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR)</a>, working with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>. 
    My current research focuses on machine learning and solving real-world sequential decision-making problems (planning/RL), such as robotics, scientific discovery and adaptive clean technology. The specifics of this research have covered the areas of human-robot collaboration, generalization, reinforcement learning, continual learning, meta-learning, multi-agent learning, and hierarchical learning. I also teach courses on data science and robot learning at Université de Montréal and Mila, covering the most recent research on machine learning techniques for creating generalist agents. I have also co-created a <a href="https://rl-conference.cc/">new conference for reinforcement learning research</a>.  

<p>For a more formal biography, click <a href="biography/bio.html">here</a> </p>   
      </td>	</tr> 
 .
    </table></div>

<ul>
  <li>I support <a href="http://slow-science.org/">Slow Science</a>.</li>
</ul>



<h2>News</h2>






<ul>
  <li>
    <p>(January 2025) Three new papers accepted to ICLR 2025 on <a href="http://arxiv.org/abs/2411.07007">efficient imitation learning</a>, <a href="https://openreview.net/forum?id=HH4KWP8RP5">exploration for GFNs</a>, and <a href="https://arxiv.org/abs/2412.14355">realtime reinforcementlearning</a>!</p>
  </li>
  <li>
    <p>(November 2024) Three new papers accepted to NeurIPS 2024 on <a href="https://openreview.net/forum?id=T5Cerv7PT2">inverse safe RL</a>, <a href="http://arxiv.org/abs/2409.04792">converging and scaling deepRL algorithms</a>, and <a href="http://arxiv.org/abs/2405.20971">amortizing intractable inference</a>!</p>
  </li>
  <li>
    <p>(August 2024) The first <a href="https://rl-conference.cc/">Conference on Reinforecment Learning</a> was a blast!</p>
  </li>
  <li>
    <p>(June 2024) New paper accepted to RSS 2024 on <a href="https://droid-dataset.github.io/">Making more diverse data sets for improving generalist robot policies</a>!</p>
  </li>
  <li>
    <p>(May 2024) Best paper award at ICLR to ICLR 2024 on <a href="https://robotics-transformer-x.github.io/">VLMs for robotics</a>!</p>
  </li>
  <li>
    <p>(January 2024) Five new papers accepted to ICLR 2024 on <a href="https://arxiv.org/abs/2310.18144">efficient exploration</a>, <a href="https://arxiv.org/abs/2401.11237">generalization in sequence planning for supervised learning</a>, <a href="https://openreview.net/forum?id=Nq45xeghcL">Intelligent Switching for Reset-Free RL</a>, <a href="https://arxiv.org/abs/2310.02902">Using LLMs and RL for drug discovery</a>, and <a href="https://arxiv.org/abs/2309.06599">Latent diffusion for OfflineRL</a>!</p>
  </li>
  <li>
    <p>(September 2023) New paper accepted to NeurIPS 2023 on <a href="https://arxiv.org/abs/2306.14808">efficient exploration in finite horizons</a>!</p>
  </li>
  <li>
    <p>(September 2023) New paper accepted to <a href="https://arxiv.org/abs/2309.03839">iROS</a> on using OfflineRL to bootstrap human robot interfaces!</p>
  </li>
  <li>
    <p>(August 2023) New paper accepted to <a href="https://ieeexplore.ieee.org/document/10214627">RA-L</a> on the benefits of torque control over position control for humanoid robots and Sim2Real training!</p>
  </li>
  <li>
    <p>(June 2022) <a href="https://arxiv.org/abs/2208.01160">New paper at IROS on quadruped robots learning soccer shooting skills.</a> IROS Best RoboCup Paper Award Finalist!</p>
  </li>
  <li>
    <p>(September 2021) New paper accepted to NeurIPS on <a href="https://sites.google.com/view/ic2/home">surprise minimization in partially observed environments</a>!</p>
  </li>
  <li>
    <p>(April 2021) Our <a href="https://arxiv.org/abs/2103.14295">research paper</a> that will be presented at ICRA2021 on <a href="https://youtu.be/goxCjGPQH7U">RL for bipedal robots</a> was featured in <a href="https://www.technologyreview.com/2021/04/08/1022176/boston-dynamics-cassie-robot-walk-reinforcement-learning-ai/">MIT Technology Review</a></p>
  </li>
</ul>




<h2>Research Talk (2023)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y20ICd1voEM?si=eCA1vVwSlgEHS9Qt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2>Recent Research Talk (2023)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/-7sctG9f1As?si=h9WnQrip-8_vc0Tk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


<h2 id="recent-work">Recent Work</h2>

<div align="center">     <table align="center">        <tr>    <td width="33%">  <a href="https://sites.google.com/view/relmm"><h4>Autonomous real-world Reinforcement learning </h4><img width="100%" src="/assets/projects/ReLMM/complex_room_short.gif" />
</a>
 </td> <td width="66%">  
We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of autonomous real-world training.
    </td>	</tr> </table></div>

<div align="center">     <table align="center">        <tr>    <td width="33%"> 
<a href="https://arxiv.org/abs/2104.11707">
<h4>Generalization across robots via inferring morphology</h4>
  <img width="100%" src="/assets/projects/anymorph/anymorph_prompt.gif" /> 
  </a>
  </td> <td width="66%"> 
The prototypical approach to reinforcement learning involves training policies tailored to a particular agent from scratch for every new morphology. Recent work aims to eliminate the re-training of policies by investigating whether a morphology-agnostic policy, trained on a diverse set of agents with similar task objectives, can be transferred to new agents with unseen morphologies without re-training. This is a challenging problem that required previous approaches to use hand-designed descriptions of the new agent's morphology. Instead of hand-designing this description, we propose a data-driven method that learns a representation of morphology directly from the reinforcement learning objective. Ours is the first reinforcement learning algorithm that can train a policy to generalize to new agent morphologies without requiring a description of the agent's morphology in advance. We evaluate our approach on the standard benchmark for agent-agnostic control, and improve over the current state of the art in zero-shot generalization to new agents. Importantly, our method attains good performance without an explicit description of morphology. 
      </td>	</tr> </table></div>

<div align="center">     <table align="center">        <tr>    <td width="33%">   
<a href="https://sites.google.com/view/surpriseminimization">
<h4>Entropy minimization for emergent behaviour</h4>
<img width="100%" src="/assets/projects/SMiRL/robotsurprise_stacked.png" />   <img width="100%" src="/assets/projects/SMiRL/miniGrid/minigrid-maze-random-count.gif" /></a> </td> <td width="66%">
  
All living organisms carve out environmental niches within which they can maintain relative predictability amidst the ever-increasing entropy around them [schneider1994, friston2009]. Humans, for example, go to great lengths to shield themselves from surprise --- we band together in millions to build cities with homes, supplying water, food, gas, and electricity to control the deterioration of our bodies and living spaces amidst heat and cold, wind and storm. The need to discover and maintain such surprise-free equilibria has driven great resourcefulness and skill in organisms across very diverse natural habitats. Motivated by this, we ask: could the motive of preserving order amidst chaos guide the automatic acquisition of useful behaviors in artificial agents?      
      </td>	</tr> </table></div>



    </div>

</article>
        </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">
        

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li class="p-name">Glen Berseth</li></ul>
            </div>

            <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/neo-x"><svg
                class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#github"></use>
            </svg> <span class="username">neo-x</span></a></li><li><a
            href="https://www.linkedin.com/in/glen-berseth-0523278b"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
            </svg> <span class="username">glen-berseth-0523278b</span></a></li><li><a
            href="https://www.twitter.com/GlenBerseth"><svg class="svg-icon">
                <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
            </svg> <span class="username">GlenBerseth</span></a></li><li><a rel="me"
            href="https://www.youtube.com/channel/UCOouaBg4gHIlNvPkJn_8ooA"
            title="Real Lab"><svg class="svg-icon grey">
                <use xlink:href="/assets/minima-social-icons.svg#youtube"></use>
            </svg> <span class="username">Real Lab</span></a></li></ul></div>

            <div class="footer-col footer-col-3">
                <p>I (he/him/il) am an assistant professor at the University de Montreal and Mila. My research explores how to use deep learning and reinforcement learning to develop generalist robots.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>