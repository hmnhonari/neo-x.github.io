---
title: Inter-Level Cooperation in Hierarchical Reinforcement Learning
date: 2020-08-13 10:20
modified: Wednesday, 10. Dec 2019 02:06PM 
category: Publication
Tags: ReinforcementLearning, HierarchicalRL
author: Glen Berseth
authors: Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak Parajuli, Sergey Levine, and Alexandre M. Bayen
web_link: https://sites.google.com/berkeley.edu/cooperative-hrl
cover: <div align="center">       <img width="100%" src="/assets/projects/CHER/overview.png"> </div>
summary: Hierarchical models for deep reinforcement learning (RL) have emerged as powerful methods for generating meaningful control strategies in difficult long time horizon tasks. Training of said hierarchical models, however, continue to suffer from instabilities that limit their applicability. In this paper, we address instabilities that arise from the concurrent optimization of goal-assignment and goal-achievement policies. Drawing connections between this concurrent optimization scheme and communication and cooperation in multi-agent RL, we redefine the standard optimization procedure to explicitly promote cooperation between these disparate tasks. Our method is demonstrated to achieve superior results to existing techniques in a set of difficult long time horizon tasks, and serves to expand the scope of solvable tasks by hierarchical reinforcement learning. 
layout: page
Type: Hierarchical Reinforcement Learning
TitleShort: Multi-level policy optimization as MARL
---

<div align="center">
	<p>
				Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak Parajuli, Sergey Levine, and Alexandre M. Bayen
	</p>
	<p>	
            UC Berkeley
    </p>
</div>


Motivated by studies on differentiable communication and emergent cooperation phenomena in MARL, we propose a novel optimization procedure to address limitations associated with inter-level cooperation in HRL. Our approach attempts to encourage cooperation between various levels of a hierarchy by redefining the objective of higher-level policies to directly account for losses experienced by lower-level policies, thereby allowing the policy to disambiguate goals with small expected returns from goals that were unachievable by the lower-level policy. The gradients associated with these additions to the loss of the higher-level policies are then propagated through its parameters by replacing the communication actions (or goals) by the higher-level policy during training with direct connections between its output and the input to the lower-level policy (see the right figure below).


<div align="center">
            <img width="80%" src="/assets/projects/CHER/overview.png">
</div>

#Results

We demonstrate improved performance over current HRL methods across a number of difficult long term planning tasks.

## AntMaze

We present the performance of the CHER algorithm on a suite of continuous control tasks. The first of these, AntMaze, can be seen in the videos of below. In this task, the agent is tasked with reaching an arbitrary position in the maze, with the videos below representing the task of reaching each of the three corners. In this problem, both the standard HRL and CHER algorithms are capable of attaining approximately similar optimal solutions. The goals in both situations (represented by the blue ant) are also very distant from the agent and do not need to change frequently, but instead simply provide the ant with a direction of movement. The simplicity of the required higher level behavior is likely a factor explaining why inter-level cooperation is not required here.

<div class="t">
    <table align="center">
    	</tr>
        <tr align=center>
        <td>
            Normal HRL
            </td>
        <td>
            CHER
            </td>
        </tr>
        <tr>
    <td align="center">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/ak37Y0aqU0I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </td>
    <td>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/3eIsEfuC9FY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
           </td>
</table>
</div>

##AntFourRooms

In the AntFourRooms environment, we begin seeing the performative improvements that can arise from promoting inter-level cooperation between agents. In this environment, the agent is tasked to reach one of the three corners of the environment. While both standard HRL and CHER are capable of navigating to the adjacent rooms, the standard approach does not succeed in navigating to the diagonal room via any of the adjacent lane, but instead move the shortest distance, thereby colliding into the walls ahead.

<div class="t">
    <table align="center">
    	</tr>
        <tr align=center>
        <td>
            Normal HRL
            </td>
        <td>
            CHER
            </td>
        </tr>
        <tr>
    <td align="center">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/SVKuKV20_RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </td>
    <td>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/gf0LrAIrh3A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
           </td>
</table>
</div>


This post is based on the following paper:

- Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak Parajuli, Sergey Levine, and Alexandre M. Bayen. <br />
  [Inter-Level Cooperation in Hierarchical Reinforcement Learning](https://arxiv.org/abs/1912.02368) <br />
  [Project Website](https://sites.google.com/berkeley.edu/cooperative-hrl)
